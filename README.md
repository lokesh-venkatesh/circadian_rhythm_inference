This is a reconstruction of the Variational Autoencoder model on 50 years of hourly temperature data by David Kyle in his [blog](https://towardsdatascience.com/vae-for-time-series-1dc0fef4bffa/).

I have stuck to the same architecture that David Kyle did. It consists of 5 convolutional layers in the encoder, and 5 mirroring deconvolutional layers in the decoder. The latent space is fitted to a seasonal prior distribution.

I also further analysed the vectors obtained after passing the original time-series vectors through the model, by flattening them from 16 dimensions x 10 features into 160 components. I tried PCA as well as t-SNE, the plots for which may be found in the [images](images) folder in this repo. The circular distribution seen in the PCA and the t-SNE (3-D) plots is indeed indicative of time of year, as represented by the colour scheme followed which is supposed to represent day of year as a fraction of 365, and the largest principal component corresponds to an axis representing mean temperature, points for which have been coloured in a coolwarm colourscheme indicating how high or low the average temperature across several days is according to the model.

I have not tried varying the latent dimension since I need to think about how that might affect the representation in the latent space and these features, so that is one angle to further explore. Another line to explore is also to try clustering methods and see if we can fit each of these vectors into unique clusters. I also need to think about other ways to use these vectors instead of simply flattening them, maybe perhaps aggregating them along one of the two axis, or treating the matrix as a whole, although that would make it much more complicated to derive trends.

**20th May 2025**
We basically wish to get rid of the annual cycles that we see in the data. Which is those circular-ellipso-toroidal distributions as seen in [this plot](results\latent_space_pca_1_2_DOY.png). 

- If we were to train AND generate-latents using vectors from across all months, then we end up with this plot. 
- If we trained with vectors from all months but only generated latents for say Jan and Feb, then we end up with the same plot but for a subset of points.
- Now if we did both training and latent-generation using only a subset of the vectors from say Jan and Feb, then the decoder component of the model will not accurately recrete the original time series, which is a problem... **We COULD still try this though, like, training and then latent-generation on the January vectors and see what exactly happens, without bothering about the reconstruction plots and simply looking at training loss and hourly and daily autocorrelation**

*The solution then, if we want to completely eliminate the periodicity at the level of an annum, is to get rid of the mean temperature deviations arising due to seasons. This is simple. We generate a bunch of input time-series vectors, and then subtract the mean temperature from the vector, and then add a dummy value to 'mimic' the average temperature. This would give us a bunch of time series vectors that do not have any mean seasonal deviation. This also means that we can then just get rid of the $\theta$-component in the prior distribution and go with a normal Gaussian distribution, and then see what happens. We should expect to see the annual periodicity disappear, and get the true latent space corresponding to the daily rhythms*