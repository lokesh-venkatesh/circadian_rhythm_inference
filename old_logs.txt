(base) PS C:\Users\lokes\Desktop\circadian_rhythms\david_kyle_vae_model> python .\config.py

--- Running data.py ---
offset:  23.27571895
scale:   9.89200131
--- Finished data.py ---

--- Running train.py ---
2025-06-08 16:41:41.787711: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From C:\Users\lokes\anaconda3\Lib\site-packages\keras\src\backend\tensorflow\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Model: "encoder"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)      │ (None, 1536)              │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ reshape (Reshape)             │ (None, 1536, 1)           │               0 │ input_layer[0][0]          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d (Conv1D)               │ (None, 512, 20)           │             120 │ reshape[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_1 (Conv1D)             │ (None, 256, 20)           │           1,220 │ conv1d[0][0]               │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_2 (Conv1D)             │ (None, 128, 20)           │           1,220 │ conv1d_1[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_3 (Conv1D)             │ (None, 64, 20)            │           1,220 │ conv1d_2[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_4 (Conv1D)             │ (None, 32, 20)            │           1,220 │ conv1d_3[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_5 (Conv1D)             │ (None, 16, 20)            │           1,220 │ conv1d_4[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item (GetItem)            │ (None, 16, 10)            │               0 │ conv1d_5[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_1 (GetItem)          │ (None, 16, 10)            │               0 │ conv1d_5[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ sampling (Sampling)           │ (None, 16, 10)            │               0 │ get_item[0][0],            │
│                               │                           │                 │ get_item_1[0][0]           │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 6,220 (24.30 KB)
 Trainable params: 6,220 (24.30 KB)
 Non-trainable params: 0 (0.00 B)
Model: "decoder"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)           │ (None, None, 10)            │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose (Conv1DTranspose)   │ (None, None, 20)            │             620 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_1 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_2 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_3 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_4 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_5 (Conv1DTranspose) │ (None, None, 1)             │             101 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ reshape_1 (Reshape)                  │ (None, None)                │               0 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 5,601 (21.88 KB)
 Trainable params: 5,601 (21.88 KB)
 Non-trainable params: 0 (0.00 B)
Model: "seasonal_prior"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer_2 (InputLayer)    │ (None, None, 6)           │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense (Dense)                 │ (None, None, 20)          │             120 │ input_layer_2[0][0]        │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_2 (GetItem)          │ (None, None, 10)          │               0 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_3 (GetItem)          │ (None, None, 10)          │               0 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ sampling_1 (Sampling)         │ (None, None, 10)          │               0 │ get_item_2[0][0],          │
│                               │                           │                 │ get_item_3[0][0]           │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 120 (480.00 B)
 Trainable params: 120 (480.00 B)
 Non-trainable params: 0 (0.00 B)
Epoch 1/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 3s 29ms/step - loss: 1.3824 - val_kl: 0.0207 - val_loss: 1.3987 - val_recon: 1.3780
Epoch 2/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.2986 - val_kl: 0.0283 - val_loss: 1.3230 - val_recon: 1.2947
Epoch 3/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.1866 - val_kl: 0.0249 - val_loss: 1.2069 - val_recon: 1.1821
Epoch 4/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.0300 - val_kl: 0.0294 - val_loss: 0.9702 - val_recon: 0.9408
Epoch 5/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.9178 - val_kl: 0.0266 - val_loss: 0.9293 - val_recon: 0.9027
Epoch 6/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: 0.8840 - val_kl: 0.0279 - val_loss: 0.8955 - val_recon: 0.8676
Epoch 7/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: 0.8582 - val_kl: 0.0260 - val_loss: 0.8733 - val_recon: 0.8473
Epoch 8/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.8356 - val_kl: 0.0252 - val_loss: 0.8447 - val_recon: 0.8195
Epoch 9/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: 0.8147 - val_kl: 0.0237 - val_loss: 0.8326 - val_recon: 0.8089
Epoch 10/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: 0.7952 - val_kl: 0.0229 - val_loss: 0.8083 - val_recon: 0.7854
Epoch 11/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.7773 - val_kl: 0.0231 - val_loss: 0.7905 - val_recon: 0.7674
Epoch 12/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7598 - val_kl: 0.0226 - val_loss: 0.7730 - val_recon: 0.7504
Epoch 13/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.7425 - val_kl: 0.0220 - val_loss: 0.7584 - val_recon: 0.7364
Epoch 14/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7256 - val_kl: 0.0222 - val_loss: 0.7409 - val_recon: 0.7187
Epoch 15/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7084 - val_kl: 0.0221 - val_loss: 0.7254 - val_recon: 0.7033
Epoch 16/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6927 - val_kl: 0.0218 - val_loss: 0.7040 - val_recon: 0.6822
Epoch 17/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6767 - val_kl: 0.0226 - val_loss: 0.6864 - val_recon: 0.6637
Epoch 18/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6614 - val_kl: 0.0218 - val_loss: 0.6736 - val_recon: 0.6518
Epoch 19/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6469 - val_kl: 0.0231 - val_loss: 0.6573 - val_recon: 0.6343
Epoch 20/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6313 - val_kl: 0.0231 - val_loss: 0.6439 - val_recon: 0.6208
Epoch 21/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6166 - val_kl: 0.0235 - val_loss: 0.6266 - val_recon: 0.6030
Epoch 22/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6021 - val_kl: 0.0235 - val_loss: 0.6133 - val_recon: 0.5898
Epoch 23/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.5879 - val_kl: 0.0243 - val_loss: 0.5979 - val_recon: 0.5736
Epoch 24/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.5737 - val_kl: 0.0250 - val_loss: 0.5813 - val_recon: 0.5564
Epoch 25/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.5595 - val_kl: 0.0254 - val_loss: 0.5713 - val_recon: 0.5459
Epoch 26/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.5457 - val_kl: 0.0254 - val_loss: 0.5540 - val_recon: 0.5286
Epoch 27/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.5318 - val_kl: 0.0263 - val_loss: 0.5394 - val_recon: 0.5131
Epoch 28/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.5182 - val_kl: 0.0255 - val_loss: 0.5257 - val_recon: 0.5002
Epoch 29/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: 0.5046 - val_kl: 0.0266 - val_loss: 0.5133 - val_recon: 0.4867
Epoch 30/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - loss: 0.4906 - val_kl: 0.0271 - val_loss: 0.4985 - val_recon: 0.4714
Epoch 31/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.4768 - val_kl: 0.0274 - val_loss: 0.4857 - val_recon: 0.4584
Epoch 32/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.4644 - val_kl: 0.0276 - val_loss: 0.4694 - val_recon: 0.4418
Epoch 33/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: 0.4507 - val_kl: 0.0286 - val_loss: 0.4578 - val_recon: 0.4291
Epoch 34/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: 0.4370 - val_kl: 0.0285 - val_loss: 0.4425 - val_recon: 0.4139
Epoch 35/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: 0.4246 - val_kl: 0.0287 - val_loss: 0.4298 - val_recon: 0.4011
Epoch 36/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - loss: 0.4111 - val_kl: 0.0291 - val_loss: 0.4127 - val_recon: 0.3835
Epoch 37/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 0.3979 - val_kl: 0.0293 - val_loss: 0.4006 - val_recon: 0.3713
Epoch 38/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 0.3847 - val_kl: 0.0296 - val_loss: 0.3891 - val_recon: 0.3595
Epoch 39/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 0.3718 - val_kl: 0.0305 - val_loss: 0.3776 - val_recon: 0.3471
Epoch 40/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 0.3592 - val_kl: 0.0312 - val_loss: 0.3612 - val_recon: 0.3301
Epoch 41/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 0.3469 - val_kl: 0.0318 - val_loss: 0.3484 - val_recon: 0.3167
Epoch 42/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 0.3339 - val_kl: 0.0325 - val_loss: 0.3351 - val_recon: 0.3026
Epoch 43/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: 0.3211 - val_kl: 0.0326 - val_loss: 0.3286 - val_recon: 0.2960
Epoch 44/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - loss: 0.3092 - val_kl: 0.0333 - val_loss: 0.3089 - val_recon: 0.2756
Epoch 45/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.2958 - val_kl: 0.0335 - val_loss: 0.2950 - val_recon: 0.2615
Epoch 46/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.2827 - val_kl: 0.0338 - val_loss: 0.2815 - val_recon: 0.2477
Epoch 47/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.2702 - val_kl: 0.0353 - val_loss: 0.2716 - val_recon: 0.2363
Epoch 48/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: 0.2578 - val_kl: 0.0366 - val_loss: 0.2550 - val_recon: 0.2184
Epoch 49/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.2451 - val_kl: 0.0368 - val_loss: 0.2425 - val_recon: 0.2057
Epoch 50/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.2323 - val_kl: 0.0366 - val_loss: 0.2330 - val_recon: 0.1964
Epoch 51/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.2204 - val_kl: 0.0374 - val_loss: 0.2165 - val_recon: 0.1791
Epoch 52/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.2081 - val_kl: 0.0381 - val_loss: 0.2049 - val_recon: 0.1668
Epoch 53/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: 0.1968 - val_kl: 0.0386 - val_loss: 0.1928 - val_recon: 0.1542
Epoch 54/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: 0.1859 - val_kl: 0.0400 - val_loss: 0.1855 - val_recon: 0.1455
Epoch 55/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.1727 - val_kl: 0.0405 - val_loss: 0.1688 - val_recon: 0.1283
Epoch 56/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: 0.1592 - val_kl: 0.0406 - val_loss: 0.1546 - val_recon: 0.1140
Epoch 57/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: 0.1474 - val_kl: 0.0416 - val_loss: 0.1420 - val_recon: 0.1004
Epoch 58/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 0.1356 - val_kl: 0.0416 - val_loss: 0.1319 - val_recon: 0.0903
Epoch 59/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 0.1238 - val_kl: 0.0426 - val_loss: 0.1180 - val_recon: 0.0754
Epoch 60/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.1123 - val_kl: 0.0428 - val_loss: 0.1065 - val_recon: 0.0637
Epoch 61/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.1002 - val_kl: 0.0439 - val_loss: 0.0926 - val_recon: 0.0487
Epoch 62/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 0.0889 - val_kl: 0.0436 - val_loss: 0.0822 - val_recon: 0.0386
Epoch 63/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: 0.0770 - val_kl: 0.0455 - val_loss: 0.0670 - val_recon: 0.0216
Epoch 64/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: 0.0668 - val_kl: 0.0462 - val_loss: 0.0593 - val_recon: 0.0131
Epoch 65/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.0561 - val_kl: 0.0458 - val_loss: 0.0480 - val_recon: 0.0022
Epoch 66/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: 0.0462 - val_kl: 0.0468 - val_loss: 0.0354 - val_recon: -0.0115
Epoch 67/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.0323 - val_kl: 0.0470 - val_loss: 0.0183 - val_recon: -0.0287
Epoch 68/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.0200 - val_kl: 0.0478 - val_loss: 0.0092 - val_recon: -0.0387
Epoch 69/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.0090 - val_kl: 0.0488 - val_loss: 0.0016 - val_recon: -0.0472
Epoch 70/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.0022 - val_kl: 0.0485 - val_loss: -0.0121 - val_recon: -0.0606
Epoch 71/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.0119 - val_kl: 0.0499 - val_loss: -0.0234 - val_recon: -0.0733
Epoch 72/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.0241 - val_kl: 0.0514 - val_loss: -0.0375 - val_recon: -0.0889
Epoch 73/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - loss: -0.0355 - val_kl: 0.0517 - val_loss: -0.0471 - val_recon: -0.0988
Epoch 74/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.0468 - val_kl: 0.0509 - val_loss: -0.0602 - val_recon: -0.1110
Epoch 75/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.0576 - val_kl: 0.0538 - val_loss: -0.0698 - val_recon: -0.1236
Epoch 76/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.0691 - val_kl: 0.0532 - val_loss: -0.0819 - val_recon: -0.1351
Epoch 77/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.0798 - val_kl: 0.0531 - val_loss: -0.0935 - val_recon: -0.1466
Epoch 78/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - loss: -0.0904 - val_kl: 0.0553 - val_loss: -0.1049 - val_recon: -0.1602
Epoch 79/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.1002 - val_kl: 0.0541 - val_loss: -0.1181 - val_recon: -0.1722
Epoch 80/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.1118 - val_kl: 0.0565 - val_loss: -0.1256 - val_recon: -0.1821
Epoch 81/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.1233 - val_kl: 0.0574 - val_loss: -0.1353 - val_recon: -0.1927
Epoch 82/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.1319 - val_kl: 0.0565 - val_loss: -0.1489 - val_recon: -0.2054
Epoch 83/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.1408 - val_kl: 0.0582 - val_loss: -0.1603 - val_recon: -0.2185
Epoch 84/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.1541 - val_kl: 0.0589 - val_loss: -0.1682 - val_recon: -0.2272
Epoch 85/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: -0.1624 - val_kl: 0.0596 - val_loss: -0.1763 - val_recon: -0.2359
Epoch 86/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.1741 - val_kl: 0.0602 - val_loss: -0.1908 - val_recon: -0.2510
Epoch 87/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.1838 - val_kl: 0.0619 - val_loss: -0.2039 - val_recon: -0.2658
Epoch 88/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: -0.1939 - val_kl: 0.0611 - val_loss: -0.2120 - val_recon: -0.2731
Epoch 89/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: -0.2047 - val_kl: 0.0619 - val_loss: -0.2225 - val_recon: -0.2844
Epoch 90/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: -0.2138 - val_kl: 0.0629 - val_loss: -0.2337 - val_recon: -0.2967
Epoch 91/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: -0.2238 - val_kl: 0.0634 - val_loss: -0.2421 - val_recon: -0.3056
Epoch 92/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: -0.2340 - val_kl: 0.0654 - val_loss: -0.2533 - val_recon: -0.3186
Epoch 93/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: -0.2432 - val_kl: 0.0653 - val_loss: -0.2643 - val_recon: -0.3297
Epoch 94/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: -0.2530 - val_kl: 0.0663 - val_loss: -0.2745 - val_recon: -0.3408
Epoch 95/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 35ms/step - loss: -0.2636 - val_kl: 0.0670 - val_loss: -0.2836 - val_recon: -0.3506
Epoch 96/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 34ms/step - loss: -0.2696 - val_kl: 0.0685 - val_loss: -0.2912 - val_recon: -0.3596
Epoch 97/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: -0.2804 - val_kl: 0.0686 - val_loss: -0.2970 - val_recon: -0.3657
Epoch 98/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: -0.2897 - val_kl: 0.0696 - val_loss: -0.3086 - val_recon: -0.3783
Epoch 99/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 36ms/step - loss: -0.2988 - val_kl: 0.0708 - val_loss: -0.3191 - val_recon: -0.3899
Epoch 100/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: -0.3082 - val_kl: 0.0718 - val_loss: -0.3268 - val_recon: -0.3986
Epoch 101/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: -0.3165 - val_kl: 0.0732 - val_loss: -0.3376 - val_recon: -0.4108
Epoch 102/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: -0.3247 - val_kl: 0.0727 - val_loss: -0.3477 - val_recon: -0.4205
Epoch 103/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.3317 - val_kl: 0.0735 - val_loss: -0.3589 - val_recon: -0.4324
Epoch 104/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.3388 - val_kl: 0.0747 - val_loss: -0.3577 - val_recon: -0.4324
Epoch 105/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.3419 - val_kl: 0.0751 - val_loss: -0.3723 - val_recon: -0.4474
Epoch 106/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.3479 - val_kl: 0.0771 - val_loss: -0.3841 - val_recon: -0.4612
Epoch 107/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.3683 - val_kl: 0.0783 - val_loss: -0.3931 - val_recon: -0.4714
Epoch 108/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: -0.3781 - val_kl: 0.0788 - val_loss: -0.4020 - val_recon: -0.4808
Epoch 109/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.3848 - val_kl: 0.0799 - val_loss: -0.4080 - val_recon: -0.4878
Epoch 110/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.3948 - val_kl: 0.0812 - val_loss: -0.4180 - val_recon: -0.4992
Epoch 111/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.4039 - val_kl: 0.0818 - val_loss: -0.4312 - val_recon: -0.5130
Epoch 112/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.4118 - val_kl: 0.0827 - val_loss: -0.4390 - val_recon: -0.5217
Epoch 113/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.4205 - val_kl: 0.0842 - val_loss: -0.4472 - val_recon: -0.5314
Epoch 114/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 16ms/step - loss: -0.4270 - val_kl: 0.0858 - val_loss: -0.4552 - val_recon: -0.5409
Epoch 115/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.4363 - val_kl: 0.0871 - val_loss: -0.4627 - val_recon: -0.5499
Epoch 116/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.4415 - val_kl: 0.0875 - val_loss: -0.4640 - val_recon: -0.5515
Epoch 117/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.4522 - val_kl: 0.0885 - val_loss: -0.4795 - val_recon: -0.5680
Epoch 118/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.4600 - val_kl: 0.0903 - val_loss: -0.4830 - val_recon: -0.5732
Epoch 119/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.4645 - val_kl: 0.0912 - val_loss: -0.4959 - val_recon: -0.5871
Epoch 120/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.4740 - val_kl: 0.0927 - val_loss: -0.4985 - val_recon: -0.5912
Epoch 121/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: -0.4823 - val_kl: 0.0931 - val_loss: -0.5085 - val_recon: -0.6015
Epoch 122/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.4865 - val_kl: 0.0946 - val_loss: -0.5153 - val_recon: -0.6098
Epoch 123/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.4931 - val_kl: 0.0958 - val_loss: -0.5186 - val_recon: -0.6144
Epoch 124/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5021 - val_kl: 0.0943 - val_loss: -0.5289 - val_recon: -0.6232
Epoch 125/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5045 - val_kl: 0.0976 - val_loss: -0.5304 - val_recon: -0.6279
Epoch 126/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5112 - val_kl: 0.0986 - val_loss: -0.5316 - val_recon: -0.6302
Epoch 127/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.5172 - val_kl: 0.0980 - val_loss: -0.5525 - val_recon: -0.6504
Epoch 128/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5227 - val_kl: 0.1001 - val_loss: -0.5548 - val_recon: -0.6549
Epoch 129/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5298 - val_kl: 0.1010 - val_loss: -0.5575 - val_recon: -0.6586
Epoch 130/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: -0.5329 - val_kl: 0.1001 - val_loss: -0.5622 - val_recon: -0.6623
Epoch 131/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5374 - val_kl: 0.1016 - val_loss: -0.5609 - val_recon: -0.6625
Epoch 132/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.5354 - val_kl: 0.1031 - val_loss: -0.5829 - val_recon: -0.6860
Epoch 133/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5568 - val_kl: 0.1030 - val_loss: -0.5906 - val_recon: -0.6937
Epoch 134/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: -0.5564 - val_kl: 0.1024 - val_loss: -0.5898 - val_recon: -0.6922
Epoch 135/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5601 - val_kl: 0.1045 - val_loss: -0.6023 - val_recon: -0.7068
Epoch 136/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: -0.5730 - val_kl: 0.1058 - val_loss: -0.6060 - val_recon: -0.7118
Epoch 137/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5731 - val_kl: 0.1063 - val_loss: -0.6130 - val_recon: -0.7193
Epoch 138/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5771 - val_kl: 0.1064 - val_loss: -0.6180 - val_recon: -0.7244
Epoch 139/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5813 - val_kl: 0.1075 - val_loss: -0.6185 - val_recon: -0.7261
Epoch 140/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5894 - val_kl: 0.1091 - val_loss: -0.6261 - val_recon: -0.7352
Epoch 141/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.5938 - val_kl: 0.1092 - val_loss: -0.6313 - val_recon: -0.7405
Epoch 142/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.5990 - val_kl: 0.1096 - val_loss: -0.6389 - val_recon: -0.7485
Epoch 143/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.6045 - val_kl: 0.1102 - val_loss: -0.6438 - val_recon: -0.7540
Epoch 144/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - loss: -0.6075 - val_kl: 0.1104 - val_loss: -0.6389 - val_recon: -0.7492
Epoch 145/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6096 - val_kl: 0.1130 - val_loss: -0.6505 - val_recon: -0.7635
Epoch 146/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6142 - val_kl: 0.1122 - val_loss: -0.6499 - val_recon: -0.7621
Epoch 147/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6186 - val_kl: 0.1137 - val_loss: -0.6535 - val_recon: -0.7672
Epoch 148/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6217 - val_kl: 0.1120 - val_loss: -0.6597 - val_recon: -0.7717
Epoch 149/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: -0.6257 - val_kl: 0.1135 - val_loss: -0.6673 - val_recon: -0.7808
Epoch 150/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6282 - val_kl: 0.1149 - val_loss: -0.6527 - val_recon: -0.7676
Epoch 151/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6319 - val_kl: 0.1152 - val_loss: -0.6708 - val_recon: -0.7859
Epoch 152/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: -0.6332 - val_kl: 0.1154 - val_loss: -0.6559 - val_recon: -0.7713
Epoch 153/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6317 - val_kl: 0.1171 - val_loss: -0.6649 - val_recon: -0.7820
Epoch 154/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6389 - val_kl: 0.1167 - val_loss: -0.6750 - val_recon: -0.7917
Epoch 155/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: -0.6439 - val_kl: 0.1193 - val_loss: -0.6777 - val_recon: -0.7970
Epoch 156/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - loss: -0.6419 - val_kl: 0.1199 - val_loss: -0.6738 - val_recon: -0.7937
Epoch 157/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - loss: -0.6456 - val_kl: 0.1202 - val_loss: -0.6832 - val_recon: -0.8034
Epoch 158/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: -0.6524 - val_kl: 0.1197 - val_loss: -0.6896 - val_recon: -0.8093
Epoch 159/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.6549 - val_kl: 0.1213 - val_loss: -0.6914 - val_recon: -0.8127
Epoch 160/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.6568 - val_kl: 0.1218 - val_loss: -0.6925 - val_recon: -0.8142
Epoch 161/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.6562 - val_kl: 0.1230 - val_loss: -0.6985 - val_recon: -0.8215
Epoch 162/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.6619 - val_kl: 0.1250 - val_loss: -0.6875 - val_recon: -0.8125
Epoch 163/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: -0.6461 - val_kl: 0.1255 - val_loss: -0.6907 - val_recon: -0.8162
Epoch 164/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: -0.6619 - val_kl: 0.1242 - val_loss: -0.7046 - val_recon: -0.8289
Epoch 165/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.6690 - val_kl: 0.1259 - val_loss: -0.7048 - val_recon: -0.8306
Epoch 166/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.6688 - val_kl: 0.1270 - val_loss: -0.7020 - val_recon: -0.8290
Epoch 167/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: -0.6712 - val_kl: 0.1270 - val_loss: -0.7083 - val_recon: -0.8353
Epoch 168/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: -0.6705 - val_kl: 0.1283 - val_loss: -0.7127 - val_recon: -0.8410
Epoch 169/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: -0.6745 - val_kl: 0.1300 - val_loss: -0.7107 - val_recon: -0.8408
Epoch 170/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.6764 - val_kl: 0.1305 - val_loss: -0.7125 - val_recon: -0.8430
Epoch 171/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.6750 - val_kl: 0.1307 - val_loss: -0.7167 - val_recon: -0.8474
Epoch 172/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: -0.6750 - val_kl: 0.1303 - val_loss: -0.7173 - val_recon: -0.8477
Epoch 173/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: -0.6743 - val_kl: 0.1319 - val_loss: -0.7178 - val_recon: -0.8498
Epoch 174/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: -0.6765 - val_kl: 0.1332 - val_loss: -0.7207 - val_recon: -0.8539
Epoch 175/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: -0.6847 - val_kl: 0.1336 - val_loss: -0.7273 - val_recon: -0.8609
Epoch 176/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.6879 - val_kl: 0.1351 - val_loss: -0.7326 - val_recon: -0.8677
Epoch 177/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.6899 - val_kl: 0.1354 - val_loss: -0.7282 - val_recon: -0.8635
Epoch 178/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: -0.6872 - val_kl: 0.1358 - val_loss: -0.7294 - val_recon: -0.8652
Epoch 179/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.6899 - val_kl: 0.1355 - val_loss: -0.7326 - val_recon: -0.8680
Epoch 180/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: -0.6926 - val_kl: 0.1389 - val_loss: -0.7309 - val_recon: -0.8698
Epoch 181/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.6921 - val_kl: 0.1376 - val_loss: -0.7351 - val_recon: -0.8727
Epoch 182/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.6968 - val_kl: 0.1378 - val_loss: -0.7222 - val_recon: -0.8600
Epoch 183/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: -0.6795 - val_kl: 0.1385 - val_loss: -0.7254 - val_recon: -0.8639
Epoch 184/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.6891 - val_kl: 0.1398 - val_loss: -0.7441 - val_recon: -0.8839
Epoch 185/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - loss: -0.6995 - val_kl: 0.1408 - val_loss: -0.7434 - val_recon: -0.8842
Epoch 186/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.7008 - val_kl: 0.1413 - val_loss: -0.7511 - val_recon: -0.8924
Epoch 187/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - loss: -0.7015 - val_kl: 0.1425 - val_loss: -0.7460 - val_recon: -0.8885
Epoch 188/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.6996 - val_kl: 0.1427 - val_loss: -0.7415 - val_recon: -0.8842
Epoch 189/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.7021 - val_kl: 0.1430 - val_loss: -0.7501 - val_recon: -0.8931
Epoch 190/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.7034 - val_kl: 0.1435 - val_loss: -0.7537 - val_recon: -0.8972
Epoch 191/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.7067 - val_kl: 0.1447 - val_loss: -0.7533 - val_recon: -0.8980
Epoch 192/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.7045 - val_kl: 0.1437 - val_loss: -0.7556 - val_recon: -0.8993
Epoch 193/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.7045 - val_kl: 0.1462 - val_loss: -0.7589 - val_recon: -0.9050
Epoch 194/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.7105 - val_kl: 0.1466 - val_loss: -0.7575 - val_recon: -0.9041
Epoch 195/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: -0.7120 - val_kl: 0.1479 - val_loss: -0.7661 - val_recon: -0.9140
Epoch 196/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: -0.7108 - val_kl: 0.1476 - val_loss: -0.7599 - val_recon: -0.9075
Epoch 197/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.7043 - val_kl: 0.1469 - val_loss: -0.7421 - val_recon: -0.8890
Epoch 198/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: -0.7114 - val_kl: 0.1492 - val_loss: -0.7660 - val_recon: -0.9152
Epoch 199/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: -0.7105 - val_kl: 0.1493 - val_loss: -0.7540 - val_recon: -0.9033
Epoch 200/200
29/29 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - loss: -0.7015 - val_kl: 0.1498 - val_loss: -0.7563 - val_recon: -0.9061
noise emission sigma:  0.10236394
Reconstruction loss:  -0.86940855
KL loss:              0.15089996
Total loss:           -0.7185086
--- Finished train.py ---

--- Running generate.py ---
2025-06-08 16:44:06.572144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From C:\Users\lokes\anaconda3\Lib\site-packages\keras\src\backend\tensorflow\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Model: "encoder"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)      │ (None, 1536)              │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ reshape (Reshape)             │ (None, 1536, 1)           │               0 │ input_layer[0][0]          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d (Conv1D)               │ (None, 512, 20)           │             120 │ reshape[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_1 (Conv1D)             │ (None, 256, 20)           │           1,220 │ conv1d[0][0]               │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_2 (Conv1D)             │ (None, 128, 20)           │           1,220 │ conv1d_1[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_3 (Conv1D)             │ (None, 64, 20)            │           1,220 │ conv1d_2[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_4 (Conv1D)             │ (None, 32, 20)            │           1,220 │ conv1d_3[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_5 (Conv1D)             │ (None, 16, 20)            │           1,220 │ conv1d_4[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item (GetItem)            │ (None, 16, 10)            │               0 │ conv1d_5[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_1 (GetItem)          │ (None, 16, 10)            │               0 │ conv1d_5[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ sampling (Sampling)           │ (None, 16, 10)            │               0 │ get_item[0][0],            │
│                               │                           │                 │ get_item_1[0][0]           │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 6,220 (24.30 KB)
 Trainable params: 6,220 (24.30 KB)
 Non-trainable params: 0 (0.00 B)
Model: "decoder"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)           │ (None, None, 10)            │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose (Conv1DTranspose)   │ (None, None, 20)            │             620 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_1 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_2 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_3 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_4 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_5 (Conv1DTranspose) │ (None, None, 1)             │             101 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ reshape_1 (Reshape)                  │ (None, None)                │               0 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 5,601 (21.88 KB)
 Trainable params: 5,601 (21.88 KB)
 Non-trainable params: 0 (0.00 B)
Model: "seasonal_prior"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer_2 (InputLayer)    │ (None, None, 6)           │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense (Dense)                 │ (None, None, 20)          │             120 │ input_layer_2[0][0]        │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_2 (GetItem)          │ (None, None, 10)          │               0 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_3 (GetItem)          │ (None, None, 10)          │               0 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ sampling_1 (Sampling)         │ (None, None, 10)          │               0 │ get_item_2[0][0],          │
│                               │                           │                 │ get_item_3[0][0]           │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 120 (480.00 B)
 Trainable params: 120 (480.00 B)
 Non-trainable params: 0 (0.00 B)
Succesfully generated time series of length: 447065
--- Finished generate.py ---

--- Running plots.py ---
1970-01-01 00:00:00    2.721405
1970-01-01 01:00:00    1.544830
1970-01-01 02:00:00    1.555817
1970-01-01 03:00:00   -0.388367
1970-01-01 04:00:00   -0.291138
Name: Observed, dtype: float64
time
1970-01-01 00:00:00    14.476311
1970-01-01 01:00:00    13.858757
1970-01-01 02:00:00    13.038679
1970-01-01 03:00:00    11.996320
1970-01-01 04:00:00    11.798944
Name: temperature, dtype: float64
--- Finished plots.py ---
(base) PS C:\Users\lokes\Desktop\circadian_rhythms\david_kyle_vae_model> python .\analysis.py

--- Running latent.py ---
2025-06-08 16:56:13.120380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From C:\Users\lokes\anaconda3\Lib\site-packages\keras\src\backend\tensorflow\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Model: "encoder"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)      │ (None, 1536)              │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ reshape (Reshape)             │ (None, 1536, 1)           │               0 │ input_layer[0][0]          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d (Conv1D)               │ (None, 512, 20)           │             120 │ reshape[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_1 (Conv1D)             │ (None, 256, 20)           │           1,220 │ conv1d[0][0]               │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_2 (Conv1D)             │ (None, 128, 20)           │           1,220 │ conv1d_1[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_3 (Conv1D)             │ (None, 64, 20)            │           1,220 │ conv1d_2[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_4 (Conv1D)             │ (None, 32, 20)            │           1,220 │ conv1d_3[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ conv1d_5 (Conv1D)             │ (None, 16, 20)            │           1,220 │ conv1d_4[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item (GetItem)            │ (None, 16, 10)            │               0 │ conv1d_5[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_1 (GetItem)          │ (None, 16, 10)            │               0 │ conv1d_5[0][0]             │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ sampling (Sampling)           │ (None, 16, 10)            │               0 │ get_item[0][0],            │
│                               │                           │                 │ get_item_1[0][0]           │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 6,220 (24.30 KB)
 Trainable params: 6,220 (24.30 KB)
 Non-trainable params: 0 (0.00 B)
Model: "decoder"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)           │ (None, None, 10)            │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose (Conv1DTranspose)   │ (None, None, 20)            │             620 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_1 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_2 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_3 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_4 (Conv1DTranspose) │ (None, None, 20)            │           1,220 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv1d_transpose_5 (Conv1DTranspose) │ (None, None, 1)             │             101 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ reshape_1 (Reshape)                  │ (None, None)                │               0 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 5,601 (21.88 KB)
 Trainable params: 5,601 (21.88 KB)
 Non-trainable params: 0 (0.00 B)
Model: "seasonal_prior"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer_2 (InputLayer)    │ (None, None, 6)           │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense (Dense)                 │ (None, None, 20)          │             120 │ input_layer_2[0][0]        │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_2 (GetItem)          │ (None, None, 10)          │               0 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item_3 (GetItem)          │ (None, None, 10)          │               0 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ sampling_1 (Sampling)         │ (None, None, 10)          │               0 │ get_item_2[0][0],          │
│                               │                           │                 │ get_item_3[0][0]           │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 120 (480.00 B)
 Trainable params: 120 (480.00 B)
 Non-trainable params: 0 (0.00 B)
--- Finished latent.py ---

--- Running analysis_scripts/flatten_pca_day_of_year.py ---
Length of the latent vectors dataset: 1161
Length of the time-series vectors dataset: 1161
--- Finished analysis_scripts/flatten_pca_day_of_year.py ---

--- Running analysis_scripts/flatten_pca_temp.py ---
Length of the latent vectors dataset: 1161
Length of the time-series vectors dataset: 1161
--- Finished analysis_scripts/flatten_pca_temp.py ---

--- Running analysis_scripts/flatten_tsne_day_of_year.py ---
Length of the latent vectors dataset: 1161
Length of the time-series vectors dataset: 1161
Running t-SNE (3D)... This may take a while.
--- Finished analysis_scripts/flatten_tsne_day_of_year.py ---

--- Running analysis_scripts/flatten_tsne_temp.py ---
Length of the latent vectors dataset: 1161
Length of the time-series vectors dataset: 1161
Running t-SNE (3D)... This may take a while.
--- Finished analysis_scripts/flatten_tsne_temp.py ---
(base) PS C:\Users\lokes\Desktop\circadian_rhythms\david_kyle_vae_model> 